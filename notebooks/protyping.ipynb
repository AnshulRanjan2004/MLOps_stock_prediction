{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDK v1 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from azureml.core import Workspace, Experiment, Environment, ScriptRunConfig\n",
    "from azureml.core.compute import AmlCompute,ComputeTarget, ComputeInstance\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.environment import CondaDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(path='../../config/config.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=Environment.from_pip_requirements(\"conv_sum\",  '../config/requirements.txt')\n",
    "env.register(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ComputeManagement import create_cluster, create_instance, delete_compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc=create_cluster(\n",
    "    workspaceRef=ws,\n",
    "    name=\"q34\",\n",
    "    vmSize=\"Standard_DS3_v2\",\n",
    "    minNodes=0,\n",
    "    maxNodes=4,\n",
    "    idleTime=180\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_script_config = ScriptRunConfig(\n",
    "#     source_directory = 'src',\n",
    "#     script = 'training_script.py',\n",
    "#     arguments=['--data',___],\n",
    "#     environment = env,\n",
    "#     compute_target = cc\n",
    "# )\n",
    "# experiment = Experiment(\n",
    "#     workspace = ws,\n",
    "#     name=\"maiden_experiment\"\n",
    "# )\n",
    "# run = experiment.submit(config=training_script_config, tags=[])\n",
    "\n",
    "# RunDetails(run).show()\n",
    "# run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensexTickerYFinance = ['HDFCLIFE.NS, NESTLEIND.NS, KOTAKBANK.NS, INDUSINDBK.NS, TATASTEEL.NS, ITC.NS, ONGC.NS, TITAN.NS, ULTRACEMCO.NS, BAJAJFINSV.NS, BAJFINANCE.NS, BRITANNIA.NS, BAJAJ-AUTO.NS, COALINDIA.NS, BHARTIARTL.NS, TATACONSUM.NS, LTI.NS, CIPLA.NS, MARUTI.NS, ICICIBANK.NS, APOLLOHOSP.NS, NTPC.NS, HEROMOTOCO.NS, HINDALCO.NS, WIPRO.NS, TCS.NS, ADANIENT.NS, MM.NS, TECHM.NS, RELIANCE.NS']\n",
    "stock_data = yf.download(tickers=sensexTickerYFinance, start='2000-01-01', end='2022-12-31', interval='1mo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.loc[:,'Adj Close']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TODO\n",
    "1. Read about SOTA for stock price prediction and what determines success of model trying to predict price\n",
    "2. Choose stocks to monitor - Nifty 50\n",
    "3. Build as below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of a common use-case where data would update regularly and model would shift\n",
    "1. Stock price prediction\n",
    "2. Automatic data retrieval using API to store into Azure storage\n",
    "3. Automatic model training at intervals depending on error rate\n",
    "\n",
    "Tie everything up in a RL portfolio optimization application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerData= yf.download(tickers=\"RELIANCE.NS\", start=\"2022-01-01\", end=\"2023-01-10\", period=\"1d\")\n",
    "tickerData['Date'] = [str(x)[:10] for x in tickerData.index]\n",
    "tickerData['Ticker'] = \"RELIANCE.NS\"\n",
    "tickerDataToPersist = list(tickerData.transpose().to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TickerData import query, download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(ticker=\"RELIANCE.NS\", start=\"2022-12-01\",end=\"2023-01-10\", period=\"1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = query(ticker=\"RELIANCE.NS\", start=\"2022-12-01\",end=\"2023-01-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker=\"RELIANCE.NS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[f\"{ticker}_Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(train_data).to_csv('./data/ril.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(series, loookaheadSize=5):\n",
    "    X,y = [],[]\n",
    "    for i in np.arange(5,len(series)-1):\n",
    "        X.append(series[i-loookaheadSize:i])\n",
    "        y.append(series[i+1])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X.reshape(len(series)-loookaheadSize-1,1,5)\n",
    "    y=y.reshape(-1,1)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
    "\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx=training_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tx,'txx.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ = torch.load('txx.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(typ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0.0026]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "str(datetime.now().date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDK v2 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from azure.ai.ml import MLClient, Input, Output, command\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml.entities import AmlCompute, Environment, Model, Data, CodeConfiguration, ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config/config.json','r') as f:\n",
    "    configs=json.loads(f.read())\n",
    "\n",
    "subscription_id, resource_group, workspace = configs['subscription_id'], configs['resource_group'], configs['workspace_name']\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "ml_client = MLClient(\n",
    "    credential, subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_cluster(\n",
    "    TargetName = \"cpu-cluster\",\n",
    "    computeSize=\"STANDARD_DS2_V2\",\n",
    "    minInstances=1,\n",
    "    maxInstances=2,\n",
    "    idleTime=180,\n",
    "    ):\n",
    "\n",
    "    try:\n",
    "        compute = ml_client.compute.get(TargetName)\n",
    "    except Exception:\n",
    "        print(\"Creating a new cpu compute target...\")\n",
    "        # Let's create the Azure ML compute object with the intended parameters\n",
    "        compute = AmlCompute(\n",
    "            name=TargetName,\n",
    "            type=\"amlcompute\",\n",
    "            size=computeSize,\n",
    "            min_instances=minInstances,\n",
    "            max_instances=maxInstances,\n",
    "            idle_time_before_scale_down=idleTime,\n",
    "        )\n",
    "        # Now, we pass the object to MLClient's create_or_update method\n",
    "        ml_client.begin_create_or_update(compute)    \n",
    "    return compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new cpu compute target...\n"
     ]
    }
   ],
   "source": [
    "compute = create_train_cluster(TargetName=\"cluster2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AmlCompute({'type': 'amlcompute', 'created_on': None, 'provisioning_state': None, 'provisioning_errors': None, 'name': 'cluster2', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': '/Users/anupam/Documents/Codebase/MLOps_stock_prediction/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x154a83390>, 'resource_id': None, 'location': 'eastus', 'size': 'STANDARD_DS2_V2', 'min_instances': 1, 'max_instances': 2, 'idle_time_before_scale_down': 180, 'identity': None, 'ssh_public_access_enabled': None, 'ssh_settings': None, 'network_settings': None, 'tier': None, 'subnet': None})"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies_dir = \"../config\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../config/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - numpy=1.21.2\n",
    "  - pip=21.2.4\n",
    "  - scipy=1.7.1\n",
    "  - pandas>=1.1,<1.2\n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]==1.3.0\n",
    "    - xlrd==2.0.1\n",
    "    - mlflow== 1.26.1\n",
    "    - azureml-mlflow==1.42.0\n",
    "    - yfinance==0.2.4\n",
    "    - pymongo==4.3.3\n",
    "    - torch==1.13.1\n",
    "    - scikit-learn==1.2.0\n",
    "    - werkzeug==0.16.1\n",
    "    - opencensus==0.11.1\n",
    "    - markupsafe==2.0.1\n",
    "    - opencensus-ext-azure==1.1.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_env(\n",
    "    dependencies_dir,\n",
    "    envName = \"stock-pred\"\n",
    "    ):\n",
    "\n",
    "    env = Environment(\n",
    "        name=envName,\n",
    "        description=\"Custom environment for creating MLOps project for stock prediction\",\n",
    "        conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
    "        version=\"0.1.6\",\n",
    "    )\n",
    "    env = ml_client.environments.create_or_update(env)\n",
    "\n",
    "    print(\n",
    "        f\"Environment with name {env.name} is registered to workspace, the environment version is {env.version}\"\n",
    "    )\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..Environment with name stock-pred is registered to workspace, the environment version is 0.1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..."
     ]
    }
   ],
   "source": [
    "env = register_env(dependencies_dir=dependencies_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data({'skip_validation': False, 'mltable_schema_url': None, 'referenced_uris': None, 'type': 'uri_file', 'is_anonymous': False, 'auto_increment_version': False, 'name': 'ril', 'description': 'ril_stock_data', 'tags': {}, 'properties': {}, 'id': '/subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourceGroups/RG/providers/Microsoft.MachineLearningServices/workspaces/AzureMLWorkspace/data/ril/versions/1', 'Resource__source_path': None, 'base_path': '/Users/anupam/Documents/Codebase/MLOps_stock_prediction/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x10ce04b10>, 'serialize': <msrest.serialization.Serializer object at 0x10ce06410>, 'version': '1', 'latest_version': None, 'path': 'azureml://subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourcegroups/RG/workspaces/AzureMLWorkspace/datastores/workspaceblobstore/paths/LocalUpload/3e91b5da3fd7435efb8a68549a7c027a/ril.csv', 'datastore': None})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_path = '../data/ril.csv'\n",
    "\n",
    "my_data = Data(\n",
    "    path=my_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"ril_stock_data\",\n",
    "    name=\"ril\",\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ril_data = ml_client.data.get(name='ril', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'azureml://subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourcegroups/RG/workspaces/AzureMLWorkspace/datastores/workspaceblobstore/paths/LocalUpload/3e91b5da3fd7435efb8a68549a7c027a/ril.csv'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ril_data.path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = \"../src/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/Train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {src_dir}Train.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import mlflow\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def series_to_tensors(series, lookaheadSize=5):\n",
    "\n",
    "    X,y = [],[]\n",
    "    for i in np.arange(5,len(series)-1):\n",
    "        X.append(series[i-lookaheadSize:i])\n",
    "        y.append(series[i+1])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X.reshape(len(series)-lookaheadSize-1,1,5)\n",
    "    y=y.reshape(-1,1)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def dataprep(args):\n",
    "\n",
    "    stockData = pd.read_csv(args.data, index_col='Unnamed: 0')\n",
    "    stock_train_df, stock_test_df = train_test_split(stockData, test_size=args.test_train_ratio)\n",
    "\n",
    "    print(stock_train_df)\n",
    "\n",
    "    # Instead of this use LayerNorm or BatchNorm in the neural net\n",
    "    scaler = MinMaxScaler().fit(stock_train_df)\n",
    "    stock_train_df = scaler.transform(stock_train_df)\n",
    "    stock_test_df = scaler.transform(stock_test_df)\n",
    "\n",
    "    train_tensors = series_to_tensors(stock_train_df)\n",
    "    test_tensors = series_to_tensors(stock_test_df)\n",
    "\n",
    "    return scaler, train_tensors, test_tensors\n",
    "\n",
    "class lstm_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(lstm_model, self).__init__()\n",
    "        self.lstm1=torch.nn.LSTM(batch_first=True, input_size=5, hidden_size=1)\n",
    "        self.out=torch.nn.Linear(1,1)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x, hidden = self.lstm1(x)\n",
    "        x = self.out(x)\n",
    "        return x.flatten()\n",
    "\n",
    "def train(trainset):\n",
    "\n",
    "    seq_model = lstm_model()\n",
    "    optim = torch.optim.Adam(lr = 0.0001, params=seq_model.parameters())\n",
    "\n",
    "    epochs = 10\n",
    "\n",
    "    for epoch in np.arange(epochs):\n",
    "\n",
    "        Loss=0\n",
    "\n",
    "        for data in trainset:\n",
    "\n",
    "            feats, target = data\n",
    "            optim.zero_grad()\n",
    "\n",
    "            y_p = seq_model(feats.float())\n",
    "            loss = torch.nn.functional.mse_loss(y_p.float(), target.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            Loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, loss: {Loss}\")\n",
    "    return seq_model\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
    "    parser.add_argument(\"--data\", type=str, help=\"Path to input data\")\n",
    "    parser.add_argument(\"--local_model_name\", type=str, required=True, default=0.25)\n",
    "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load Scaler object later and send it for scaling data\n",
    "\n",
    "    scaler, trainset, _ = dataprep(args)\n",
    "\n",
    "    trainedModel = train(trainset)\n",
    "\n",
    "    print(os.getcwd())\n",
    "\n",
    "    pickle.dump(scaler, open('./outputs/scaler.pkl','wb'))\n",
    "    model_file = f\"./outputs/{args.local_model_name}.pth\"\n",
    "    torch.save(trainedModel.state_dict(), model_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def version_iter(n=20):\n",
    "    i=0\n",
    "    for i in np.arange(21,50):\n",
    "        yield i\n",
    "x = iter(version_iter())\n",
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: the provided asset name 'stock-pred' will not be used for anonymous registration\n",
      "Warning: the provided asset name 'stock-pred' will not be used for anonymous registration\n",
      "Uploading src (0.01 MBs): 100%|██████████| 12353/12353 [00:01<00:00, 9359.10it/s] \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>train_model_stock_price_prediction</td><td>stock_pred_job_34</td><td>command</td><td>Starting</td><td><a href=\"https://ml.azure.com/runs/stock_pred_job_34?wsid=/subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourcegroups/RG/workspaces/AzureMLWorkspace&amp;tid=d390d3b6-5b59-47f2-a790-59a4e06413d3\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "Command({'parameters': {}, 'init': False, 'type': 'command', 'status': 'Starting', 'log_files': None, 'name': 'stock_pred_job_34', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/coderkol95/MLOps_stock_prediction.git', 'mlflow.source.git.branch': 'AzureML_SDK_v2', 'mlflow.source.git.commit': '1e97110641436a6680a40605a8cab6f03688b6b6', 'azureml.git.dirty': 'True', '_azureml.ComputeTargetType': 'amlctrain', 'ContentSnapshotId': '11e6cbbc-b1f3-447d-843d-4d70264c890c'}, 'id': '/subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourceGroups/RG/providers/Microsoft.MachineLearningServices/workspaces/AzureMLWorkspace/jobs/stock_pred_job_34', 'Resource__source_path': None, 'base_path': '/Users/anupam/Documents/Codebase/MLOps_stock_prediction/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x154521790>, 'serialize': <msrest.serialization.Serializer object at 0x154af2950>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'stock_price_prediction', 'experiment_name': 'train_model_stock_price_prediction', 'compute': 'cluster2', 'services': {'Tracking': <azure.ai.ml.entities._job.job_service.JobService object at 0x154e18b10>, 'Studio': <azure.ai.ml.entities._job.job_service.JobService object at 0x154d827d0>}, 'comment': None, 'job_inputs': {'data': {'type': 'uri_file', 'path': 'azureml://subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourcegroups/RG/workspaces/AzureMLWorkspace/datastores/workspaceblobstore/paths/LocalUpload/3e91b5da3fd7435efb8a68549a7c027a/ril.csv', 'mode': 'ro_mount'}, 'test_train_ratio': '0.25', 'registered_model_name': 'stock_pred_v1', 'local_model_name': 'modelstock_pred_2023-01-25'}, 'job_outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.stock_pred_job_34', 'mode': 'rw_mount'}}, 'inputs': {'data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x154aa72d0>, 'test_train_ratio': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x154d758d0>, 'registered_model_name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x154af0650>, 'local_model_name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x154af1150>}, 'outputs': {'default': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x154e91950>}, 'component': CommandComponent({'auto_increment_version': True, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': False, 'name': 'stock_pred_job_34', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': PosixPath('.'), 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x154521790>, 'serialize': <msrest.serialization.Serializer object at 0x154de5310>, 'command': 'python Train.py --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} --local_model_name ${{inputs.local_model_name}} --registered_model_name ${{inputs.registered_model_name}}', 'code': '/subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourceGroups/RG/providers/Microsoft.MachineLearningServices/workspaces/AzureMLWorkspace/codes/31575759-8cdc-4de6-8b60-d16ea546472c/versions/1', 'environment_variables': {}, 'environment': '/subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourceGroups/RG/providers/Microsoft.MachineLearningServices/workspaces/AzureMLWorkspace/environments/stock-pred/versions/0.1.6', 'distribution': None, 'resources': None, 'version': None, 'latest_version': None, 'schema': None, 'type': 'command', 'display_name': 'stock_price_prediction', 'is_deterministic': True, 'inputs': {'data': {'type': 'uri_file', 'path': 'azureml://subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourcegroups/RG/workspaces/AzureMLWorkspace/datastores/workspaceblobstore/paths/LocalUpload/3e91b5da3fd7435efb8a68549a7c027a/ril.csv', 'mode': 'ro_mount'}, 'test_train_ratio': {'type': 'string', 'default': '0.25'}, 'registered_model_name': {'type': 'string', 'default': 'stock_pred_v1'}, 'local_model_name': {'type': 'string', 'default': 'modelstock_pred_2023-01-25'}}, 'outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.stock_pred_job_34', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {'status': 'Starting', 'parameters': {}}}), 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': {'Tracking': <azure.ai.ml.entities._job.job_service.JobService object at 0x154e18b10>, 'Studio': <azure.ai.ml.entities._job.job_service.JobService object at 0x154d827d0>}, 'status': 'Starting', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x154521790>}, 'instance_id': '0e959331-a45f-45af-b852-93c1ae737228', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': 'stock-pred:0.1.6', 'resources': {'instance_count': 1, 'shm_size': '2g'}, 'swept': False})"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model_name = f\"modelstock_pred_{str(datetime.now().date())}\"\n",
    "registered_model_name = \"stock_pred_v1\"\n",
    "\n",
    "job = command(   \n",
    "    name=f\"stock_pred_job_{next(x)}\",\n",
    "    inputs={\n",
    "        \"data\": Input(type=AssetTypes.URI_FILE, mode=\"ro_mount\", path=ril_data.path),\n",
    "        \"test_train_ratio\": 0.25,\n",
    "        \"registered_model_name\":registered_model_name,\n",
    "        \"local_model_name\":local_model_name\n",
    "        },\n",
    "    code=\"../src/\",  # location of source code\n",
    "    command=\"python Train.py --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} --local_model_name ${{inputs.local_model_name}} --registered_model_name ${{inputs.registered_model_name}}\",\n",
    "    environment=env,\n",
    "    compute=compute.name,\n",
    "    experiment_name=\"train_model_stock_price_prediction\",\n",
    "    display_name=\"stock_price_prediction\",\n",
    ")\n",
    "\n",
    "ml_client.create_or_update(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_name=f\"modelstock_pred_{str(datetime.now().date())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modelstock_pred_2023-01-25'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "        path=f\"azureml://jobs/{job.name}/outputs/artifacts/paths/outputs/\",\n",
    "        name=\"model-path\",\n",
    "        description=\"Model created from run.\",\n",
    "        type=\"custom_model\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model({'job_name': 'stock_pred_job_34', 'is_anonymous': False, 'auto_increment_version': False, 'name': 'model-path', 'description': 'Model created from run.', 'tags': {}, 'properties': {}, 'id': '/subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourceGroups/RG/providers/Microsoft.MachineLearningServices/workspaces/AzureMLWorkspace/models/model-path/versions/1', 'Resource__source_path': None, 'base_path': '/Users/anupam/Documents/Codebase/MLOps_stock_prediction/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x154eb96d0>, 'serialize': <msrest.serialization.Serializer object at 0x154eb92d0>, 'version': '1', 'latest_version': None, 'path': 'azureml://subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourceGroups/RG/workspaces/AzureMLWorkspace/datastores/workspaceartifactstore/paths/ExperimentRun/dcid.stock_pred_job_34/outputs', 'datastore': None, 'utc_time_created': None, 'flavors': None, 'arm_type': 'model_version', 'type': 'custom_model'})"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.models.create_or_update(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src//Deployment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {src_dir}/Deployment.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "def init():\n",
    "    \n",
    "    global model\n",
    "    global scaler\n",
    "\n",
    "    class lstm_model(torch.nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super(lstm_model, self).__init__()\n",
    "            self.lstm1=torch.nn.LSTM(batch_first=True, input_size=5, hidden_size=1)\n",
    "            self.out=torch.nn.Linear(1,1)\n",
    "\n",
    "        def forward(self, x, hidden=None):\n",
    "            x, hidden = self.lstm1(x)\n",
    "            x = self.out(x)\n",
    "            return x.flatten()\n",
    "\n",
    "    scalerpath = os.path.join(\n",
    "    os.getenv(\"AZUREML_MODEL_DIR\"), \"outputs/scaler.pkl\")\n",
    "    print(scalerpath)\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    with open(scalerpath, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    modelpath = os.path.join(\n",
    "    os.getenv(\"AZUREML_MODEL_DIR\"), \"outputs/modelstock_pred_2023-01-25.pth\")    \n",
    "    print(modelpath)\n",
    "    model = lstm_model()\n",
    "    model = model.load_state_dict(torch.load(modelpath))\n",
    "\n",
    "def run(raw_data):\n",
    "\n",
    "    data = json.loads(raw_data)[\"data\"]\n",
    "    data = np.array(data)\n",
    "    scaled_data = scaler.transform(data)\n",
    "\n",
    "    result = model.predict(scaled_data)\n",
    "\n",
    "    return result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x154e9c490>"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create endpoint\n",
    "online_endpoint_name = \"modeldir-logged-init\"\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"this is a sample online endpoint\"\n",
    ")\n",
    "\n",
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Succeeded'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_endpoints.get(name=online_endpoint_name).provisioning_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "modeldir = ml_client.models.get(name=\"model-path\", version=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create deployment only after endpoint has provisioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint modeldir-logged-init exists\n",
      "Uploading src (0.01 MBs): 100%|██████████| 12645/12645 [00:01<00:00, 9572.42it/s]\n",
      "\n",
      "\n",
      "data_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x154e62650>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................................................................."
     ]
    }
   ],
   "source": [
    "# Deployment script\n",
    "code_config = CodeConfiguration(\n",
    "        code=src_dir, scoring_script=\"Deployment.py\"\n",
    "    )\n",
    "\n",
    "# Create deployment\n",
    "yellow_deployment = ManagedOnlineDeployment(\n",
    "    name=\"yellow\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=modeldir,\n",
    "    environment=Environment(\n",
    "            conda_file=f\"./{dependencies_dir}/conda.yml\",\n",
    "            image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\"),\n",
    "    code_configuration=code_config,\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "# create the deployment:\n",
    "ml_client.begin_create_or_update(yellow_deployment)\n",
    "# blue deployment takes 100 traffic\n",
    "# endpoint.traffic = {\"yellow\": 100}\n",
    "# ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instance status:\\nSystemSetup: Succeeded\\nUserContainerImagePull: Succeeded\\nModelDownload: Succeeded\\nUserContainerStart: Succeeded\\n\\nContainer events:\\nKind: Pod, Name: Pulling, Type: Normal, Time: 2023-01-25T03:40:18.439511Z, Message: Start pulling container image\\nKind: Pod, Name: Downloading, Type: Normal, Time: 2023-01-25T03:40:18.63363Z, Message: Start downloading models\\nKind: Pod, Name: Pulled, Type: Normal, Time: 2023-01-25T03:40:33.912731Z, Message: Container image is pulled successfully\\nKind: Pod, Name: Downloaded, Type: Normal, Time: 2023-01-25T03:40:33.912731Z, Message: Models are downloaded successfully\\nKind: Pod, Name: Created, Type: Normal, Time: 2023-01-25T03:40:34.098431Z, Message: Created container inference-server\\nKind: Pod, Name: Started, Type: Normal, Time: 2023-01-25T03:40:34.243024Z, Message: Started container inference-server\\nKind: Pod, Name: ContainerReady, Type: Normal, Time: 2023-01-25T03:40:53.777998255Z, Message: Container is ready\\n\\nContainer logs:\\nsqlparse==0.4.3\\ntabulate==0.9.0\\nthreadpoolctl==3.1.0\\ntorch==1.13.1\\ntyping_extensions==4.4.0\\nurllib3==1.26.9\\nwebencodings==0.5.1\\nwebsocket-client==1.4.2\\nWerkzeug==0.16.1\\nwrapt==1.12.1\\nxlrd==2.0.1\\nyfinance==0.2.4\\nzipp==3.11.0\\n\\n2023-01-25T03:40:35,233274592+00:00 | gunicorn/run | \\n2023-01-25T03:40:35,234634021+00:00 | gunicorn/run | ###############################################\\n2023-01-25T03:40:35,236266854+00:00 | gunicorn/run | AzureML Inference Server\\n2023-01-25T03:40:35,237507280+00:00 | gunicorn/run | ###############################################\\n2023-01-25T03:40:35,238831807+00:00 | gunicorn/run | \\n2023-01-25T03:40:35,559964651+00:00 | gunicorn/run | \\n2023-01-25T03:40:35,561398280+00:00 | gunicorn/run | Starting HTTP server\\n2023-01-25T03:40:35,562902511+00:00 | gunicorn/run | \\nStarting gunicorn 20.1.0\\nListening at: http://127.0.0.1:31311 (12)\\nUsing worker: sync\\nworker timeout is set to 300\\nBooting worker with pid: 67\\nInitializing logger\\n2023-01-25 03:40:36,354 | root | INFO | Starting up app insights client\\nlogging socket was found. logging is available.\\nlogging socket was found. logging is available.\\n2023-01-25 03:40:36,355 | root | INFO | Starting up app insight hooks\\n2023-01-25 03:40:39,004 | root | INFO | Found user script at /var/azureml-app/src/Deployment.py\\n2023-01-25 03:40:39,004 | root | INFO | run() is not decorated. Server will invoke it with the input in JSON string.\\n2023-01-25 03:40:39,004 | root | INFO | Invoking user\\'s init function\\n00000000-0000-0000-0000-000000000000,/var/azureml-app/azureml-models/model-path/1/outputs/scaler.pkl\\n00000000-0000-0000-0000-000000000000,/var/azureml-app/azureml-models/model-path/1/outputs/modelstock_pred_2023-01-25.pth\\n2023-01-25 03:40:39,589 | root | INFO | Users\\'s init has completed successfully\\n2023-01-25 03:40:39,589 | root | ERROR | Encountered exception while generating swagger file: Traceback (most recent call last):\\n  File \"/var/azureml-server/aml_blueprint.py\", line 111, in _get_swagger\\n    from inference_schema.schema_util import (\\nImportError: cannot import name \\'get_supported_versions\\' from \\'inference_schema.schema_util\\' (/azureml-envs/azureml_886de9eab753e00d77cbed89865b3c2b/lib/python3.8/site-packages/inference_schema/schema_util.py)\\n\\n2023-01-25 03:40:39,590 | root | ERROR | Encountered exception while generating swagger file: Traceback (most recent call last):\\n  File \"/var/azureml-server/aml_blueprint.py\", line 111, in _get_swagger\\n    from inference_schema.schema_util import (\\nImportError: cannot import name \\'get_supported_versions\\' from \\'inference_schema.schema_util\\' (/azureml-envs/azureml_886de9eab753e00d77cbed89865b3c2b/lib/python3.8/site-packages/inference_schema/schema_util.py)\\n\\n2023-01-25 03:40:39,590 | root | INFO | Scoring timeout setting is not found. Use default timeout: 3600000 ms\\n2023-01-25 03:40:39,590 | root | INFO | AML_FLASK_ONE_COMPATIBILITY is set, but patching is not necessary.\\n\\n'"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_deployments.get_logs(\n",
    "    name=\"yellow\", endpoint_name=online_endpoint_name, lines=50\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputs.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile inputs.json\n",
    "{\"data1\":\"24.53\",\"data2\":\"26.544\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/azureML/lib/python3.11/site-packages/azure/ai/ml/_utils/_endpoint_utils.py:116\u001b[0m, in \u001b[0;36mvalidate_response\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     r_json \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39;49mjson()\n\u001b[1;32m    117\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39m# exception is not in the json format\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/azureML/lib/python3.11/site-packages/azure/core/rest/_http_response_impl.py:312\u001b[0m, in \u001b[0;36m_HttpResponseBaseImpl.json\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_json:\n\u001b[0;32m--> 312\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_json \u001b[39m=\u001b[39m loads(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext())\n\u001b[1;32m    313\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_json\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/azureML/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/azureML/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/azureML/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[307], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# test the endpoint (the request will route to blue deployment as set above)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ml_client\u001b[39m.\u001b[39;49monline_endpoints\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m      3\u001b[0m     endpoint_name\u001b[39m=\u001b[39;49monline_endpoint_name,\n\u001b[1;32m      4\u001b[0m     deployment_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39myellow\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     request_file\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minputs.json\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/azureML/lib/python3.11/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[39m=\u001b[39m settings\u001b[39m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m span_impl_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m \u001b[39m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m merge_span \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/azureML/lib/python3.11/site-packages/azure/ai/ml/operations/_online_endpoint_operations.py:359\u001b[0m, in \u001b[0;36mOnlineEndpointOperations.invoke\u001b[0;34m(self, endpoint_name, request_file, deployment_name, input_data, params_override, local, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m     headers[EndpointInvokeFields\u001b[39m.\u001b[39mMODEL_DEPLOYMENT] \u001b[39m=\u001b[39m deployment_name\n\u001b[1;32m    358\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_requests_pipeline\u001b[39m.\u001b[39mpost(endpoint\u001b[39m.\u001b[39mproperties\u001b[39m.\u001b[39mscoring_uri, json\u001b[39m=\u001b[39mdata, headers\u001b[39m=\u001b[39mheaders)\n\u001b[0;32m--> 359\u001b[0m validate_response(response)\n\u001b[1;32m    360\u001b[0m \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mtext()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/azureML/lib/python3.11/site-packages/azure/ai/ml/_utils/_endpoint_utils.py:119\u001b[0m, in \u001b[0;36mvalidate_response\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    116\u001b[0m         r_json \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[1;32m    117\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m         \u001b[39m# exception is not in the json format\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(response\u001b[39m.\u001b[39mcontent\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    120\u001b[0m failure_msg \u001b[39m=\u001b[39m r_json\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m, response)\n\u001b[1;32m    121\u001b[0m error_map \u001b[39m=\u001b[39m {\n\u001b[1;32m    122\u001b[0m     \u001b[39m401\u001b[39m: ClientAuthenticationError,\n\u001b[1;32m    123\u001b[0m     \u001b[39m404\u001b[39m: ResourceNotFoundError,\n\u001b[1;32m    124\u001b[0m     \u001b[39m409\u001b[39m: ResourceExistsError,\n\u001b[1;32m    125\u001b[0m }\n",
      "\u001b[0;31mException\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "# test the endpoint (the request will route to blue deployment as set above)\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"yellow\",\n",
    "    request_file=\"inputs.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': '24'}"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads('{\"data\":\"24\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "495794734634ed83a223a0ff466dd16659bfe4b87c179f1ac740ad96b06e37ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
