{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDK v1 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from azureml.core import Workspace, Experiment, Environment, ScriptRunConfig\n",
    "from azureml.core.compute import AmlCompute,ComputeTarget, ComputeInstance\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.environment import CondaDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(path='../../config/config.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=Environment.from_pip_requirements(\"conv_sum\",  '../config/requirements.txt')\n",
    "env.register(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ComputeManagement import create_cluster, create_instance, delete_compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc=create_cluster(\n",
    "    workspaceRef=ws,\n",
    "    name=\"q34\",\n",
    "    vmSize=\"Standard_DS3_v2\",\n",
    "    minNodes=0,\n",
    "    maxNodes=4,\n",
    "    idleTime=180\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_script_config = ScriptRunConfig(\n",
    "#     source_directory = 'src',\n",
    "#     script = 'training_script.py',\n",
    "#     arguments=['--data',___],\n",
    "#     environment = env,\n",
    "#     compute_target = cc\n",
    "# )\n",
    "# experiment = Experiment(\n",
    "#     workspace = ws,\n",
    "#     name=\"maiden_experiment\"\n",
    "# )\n",
    "# run = experiment.submit(config=training_script_config, tags=[])\n",
    "\n",
    "# RunDetails(run).show()\n",
    "# run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensexTickerYFinance = ['HDFCLIFE.NS, NESTLEIND.NS, KOTAKBANK.NS, INDUSINDBK.NS, TATASTEEL.NS, ITC.NS, ONGC.NS, TITAN.NS, ULTRACEMCO.NS, BAJAJFINSV.NS, BAJFINANCE.NS, BRITANNIA.NS, BAJAJ-AUTO.NS, COALINDIA.NS, BHARTIARTL.NS, TATACONSUM.NS, LTI.NS, CIPLA.NS, MARUTI.NS, ICICIBANK.NS, APOLLOHOSP.NS, NTPC.NS, HEROMOTOCO.NS, HINDALCO.NS, WIPRO.NS, TCS.NS, ADANIENT.NS, MM.NS, TECHM.NS, RELIANCE.NS']\n",
    "stock_data = yf.download(tickers=sensexTickerYFinance, start='2000-01-01', end='2022-12-31', interval='1mo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.loc[:,'Adj Close']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TODO\n",
    "1. Read about SOTA for stock price prediction and what determines success of model trying to predict price\n",
    "2. Choose stocks to monitor - Nifty 50\n",
    "3. Build as below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of a common use-case where data would update regularly and model would shift\n",
    "1. Stock price prediction\n",
    "2. Automatic data retrieval using API to store into Azure storage\n",
    "3. Automatic model training at intervals depending on error rate\n",
    "\n",
    "Tie everything up in a RL portfolio optimization application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerData= yf.download(tickers=\"RELIANCE.NS\", start=\"2022-01-01\", end=\"2023-01-10\", period=\"1d\")\n",
    "tickerData['Date'] = [str(x)[:10] for x in tickerData.index]\n",
    "tickerData['Ticker'] = \"RELIANCE.NS\"\n",
    "tickerDataToPersist = list(tickerData.transpose().to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TickerData import query, download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(ticker=\"RELIANCE.NS\", start=\"2022-12-01\",end=\"2023-01-10\", period=\"1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = query(ticker=\"RELIANCE.NS\", start=\"2022-12-01\",end=\"2023-01-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker=\"RELIANCE.NS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[f\"{ticker}_Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(train_data).to_csv('./data/ril.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(series, loookaheadSize=5):\n",
    "    X,y = [],[]\n",
    "    for i in np.arange(5,len(series)-1):\n",
    "        X.append(series[i-loookaheadSize:i])\n",
    "        y.append(series[i+1])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X.reshape(len(series)-loookaheadSize-1,1,5)\n",
    "    y=y.reshape(-1,1)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
    "\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx=training_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tx,'txx.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ = torch.load('txx.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(typ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0.0026]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "str(datetime.now().date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDK v2 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from azure.ai.ml import MLClient, Input, Output, command\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml.entities import AmlCompute, Environment, Model, Data, CodeConfiguration, ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config/config.json','r') as f:\n",
    "    configs=json.loads(f.read())\n",
    "\n",
    "subscription_id, resource_group, workspace = configs['subscription_id'], configs['resource_group'], configs['workspace_name']\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "ml_client = MLClient(\n",
    "    credential, subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_cluster(\n",
    "    TargetName = \"cpu-cluster\",\n",
    "    computeSize=\"STANDARD_DS2_V2\",\n",
    "    minInstances=1,\n",
    "    maxInstances=2,\n",
    "    idleTime=180,\n",
    "    ):\n",
    "\n",
    "    try:\n",
    "        compute = ml_client.compute.get(TargetName)\n",
    "    except Exception:\n",
    "        print(\"Creating a new cpu compute target...\")\n",
    "        # Let's create the Azure ML compute object with the intended parameters\n",
    "        compute = AmlCompute(\n",
    "            name=TargetName,\n",
    "            type=\"amlcompute\",\n",
    "            size=computeSize,\n",
    "            min_instances=minInstances,\n",
    "            max_instances=maxInstances,\n",
    "            idle_time_before_scale_down=idleTime,\n",
    "        )\n",
    "        # Now, we pass the object to MLClient's create_or_update method\n",
    "        ml_client.begin_create_or_update(compute)    \n",
    "    return compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new cpu compute target...\n"
     ]
    }
   ],
   "source": [
    "compute = create_train_cluster(TargetName=\"cluster2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AmlCompute({'type': 'amlcompute', 'created_on': None, 'provisioning_state': None, 'provisioning_errors': None, 'name': 'cluster2', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': '/Users/anupam/Documents/Codebase/MLOps_stock_prediction/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x154a83390>, 'resource_id': None, 'location': 'eastus', 'size': 'STANDARD_DS2_V2', 'min_instances': 1, 'max_instances': 2, 'idle_time_before_scale_down': 180, 'identity': None, 'ssh_public_access_enabled': None, 'ssh_settings': None, 'network_settings': None, 'tier': None, 'subnet': None})"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies_dir = \"../config\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../config/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - numpy=1.21.2\n",
    "  - pip=21.2.4\n",
    "  - scipy=1.7.1\n",
    "  - pandas>=1.1,<1.2\n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]==1.3.0\n",
    "    - xlrd==2.0.1\n",
    "    - mlflow== 1.26.1\n",
    "    - azureml-mlflow==1.42.0\n",
    "    - yfinance==0.2.4\n",
    "    - pymongo==4.3.3\n",
    "    - torch==1.13.1\n",
    "    - scikit-learn==1.2.0\n",
    "    - werkzeug==0.16.1\n",
    "    - opencensus==0.11.1\n",
    "    - markupsafe==2.0.1\n",
    "    - opencensus-ext-azure==1.1.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_env(\n",
    "    dependencies_dir,\n",
    "    envName = \"stock-pred\"\n",
    "    ):\n",
    "\n",
    "    env = Environment(\n",
    "        name=envName,\n",
    "        description=\"Custom environment for creating MLOps project for stock prediction\",\n",
    "        conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
    "        version=\"0.1.6\",\n",
    "    )\n",
    "    env = ml_client.environments.create_or_update(env)\n",
    "\n",
    "    print(\n",
    "        f\"Environment with name {env.name} is registered to workspace, the environment version is {env.version}\"\n",
    "    )\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..Environment with name stock-pred is registered to workspace, the environment version is 0.1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..."
     ]
    }
   ],
   "source": [
    "env = register_env(dependencies_dir=dependencies_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data({'skip_validation': False, 'mltable_schema_url': None, 'referenced_uris': None, 'type': 'uri_file', 'is_anonymous': False, 'auto_increment_version': False, 'name': 'ril', 'description': 'ril_stock_data', 'tags': {}, 'properties': {}, 'id': '/subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourceGroups/RG/providers/Microsoft.MachineLearningServices/workspaces/AzureMLWorkspace/data/ril/versions/1', 'Resource__source_path': None, 'base_path': '/Users/anupam/Documents/Codebase/MLOps_stock_prediction/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x10ce04b10>, 'serialize': <msrest.serialization.Serializer object at 0x10ce06410>, 'version': '1', 'latest_version': None, 'path': 'azureml://subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourcegroups/RG/workspaces/AzureMLWorkspace/datastores/workspaceblobstore/paths/LocalUpload/3e91b5da3fd7435efb8a68549a7c027a/ril.csv', 'datastore': None})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_path = '../data/ril.csv'\n",
    "\n",
    "my_data = Data(\n",
    "    path=my_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"ril_stock_data\",\n",
    "    name=\"ril\",\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ril_data = ml_client.data.get(name='ril', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'azureml://subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourcegroups/RG/workspaces/AzureMLWorkspace/datastores/workspaceblobstore/paths/LocalUpload/3e91b5da3fd7435efb8a68549a7c027a/ril.csv'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ril_data.path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = \"../src/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {src_dir}train.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import mlflow\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def series_to_tensors(series, lookaheadSize=5):\n",
    "\n",
    "    X,y = [],[]\n",
    "    for i in np.arange(5,len(series)-1):\n",
    "        X.append(series[i-lookaheadSize:i])\n",
    "        y.append(series[i+1])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X.reshape(len(series)-lookaheadSize-1,1,5)\n",
    "    y=y.reshape(-1,1)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def dataprep(args):\n",
    "\n",
    "    stockData = pd.read_csv(args.data, index_col='Unnamed: 0')\n",
    "    stock_train_df, stock_test_df = train_test_split(stockData, test_size=args.test_train_ratio)\n",
    "\n",
    "    print(stock_train_df)\n",
    "\n",
    "    # Instead of this use LayerNorm or BatchNorm in the neural net\n",
    "    scaler = MinMaxScaler().fit(stock_train_df)\n",
    "    stock_train_df = scaler.transform(stock_train_df)\n",
    "    stock_test_df = scaler.transform(stock_test_df)\n",
    "\n",
    "    train_tensors = series_to_tensors(stock_train_df)\n",
    "    test_tensors = series_to_tensors(stock_test_df)\n",
    "\n",
    "    return scaler, train_tensors, test_tensors\n",
    "\n",
    "class lstm_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(lstm_model, self).__init__()\n",
    "        self.lstm1=torch.nn.LSTM(batch_first=True, input_size=5, hidden_size=1)\n",
    "        self.out=torch.nn.Linear(1,1)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x, hidden = self.lstm1(x)\n",
    "        x = x[:,-1]\n",
    "        x = self.out(x)\n",
    "        return x, hidden\n",
    "\n",
    "def train(trainset):\n",
    "\n",
    "    seq_model = lstm_model()\n",
    "    optim = torch.optim.Adam(lr = 0.0001, params=seq_model.parameters())\n",
    "\n",
    "    epochs = 10\n",
    "\n",
    "    for epoch in np.arange(epochs):\n",
    "\n",
    "        Loss=0\n",
    "\n",
    "        for data in trainset:\n",
    "\n",
    "            feats, target = data\n",
    "            optim.zero_grad()\n",
    "\n",
    "            y_p,_ = seq_model(feats.float())\n",
    "            loss = torch.nn.functional.mse_loss(y_p.float(), target.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            Loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, loss: {Loss}\")\n",
    "    return seq_model\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
    "    parser.add_argument(\"--data\", type=str, help=\"Path to input data\")\n",
    "    parser.add_argument(\"--local_model_name\", type=str, required=True, default=0.25)\n",
    "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load Scaler object later and send it for scaling data\n",
    "\n",
    "    scaler, trainset, _ = dataprep(args)\n",
    "\n",
    "    trainedModel = train(trainset)\n",
    "\n",
    "    print(os.getcwd())\n",
    "\n",
    "    pickle.dump(scaler, open('./outputs/scaler.pkl','wb'))\n",
    "    model_file = f\"./outputs/{args.local_model_name}.pth\"\n",
    "    torch.save(trainedModel.state_dict(), model_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def version_iter(n=20):\n",
    "    i=0\n",
    "    for i in np.arange(21,50):\n",
    "        yield i\n",
    "x = iter(version_iter())\n",
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: the provided asset name 'stock-pred' will not be used for anonymous registration\n",
      "Warning: the provided asset name 'stock-pred' will not be used for anonymous registration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>train_model_stock_price_prediction</td><td>stock_pred_job_37</td><td>command</td><td>Starting</td><td><a href=\"https://ml.azure.com/runs/stock_pred_job_37?wsid=/subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourcegroups/RG/workspaces/AzureMLWorkspace&amp;tid=d390d3b6-5b59-47f2-a790-59a4e06413d3\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "Command({'parameters': {}, 'init': False, 'type': 'command', 'status': 'Starting', 'log_files': None, 'name': 'stock_pred_job_37', 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/coderkol95/MLOps_stock_prediction.git', 'mlflow.source.git.branch': 'AzureML_SDK_v2', 'mlflow.source.git.commit': '1591905eca9b09195255daf800d12abdfd736c13', 'azureml.git.dirty': 'True', '_azureml.ComputeTargetType': 'amlctrain', 'ContentSnapshotId': 'b8ace2e0-cc8e-4d21-a13a-331fc5cef744'}, 'id': '/subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourceGroups/RG/providers/Microsoft.MachineLearningServices/workspaces/AzureMLWorkspace/jobs/stock_pred_job_37', 'Resource__source_path': None, 'base_path': '/Users/anupam/Documents/Codebase/MLOps_stock_prediction/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x155721e90>, 'serialize': <msrest.serialization.Serializer object at 0x155727c90>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'stock_price_prediction', 'experiment_name': 'train_model_stock_price_prediction', 'compute': 'cluster2', 'services': {'Tracking': <azure.ai.ml.entities._job.job_service.JobService object at 0x15574fc50>, 'Studio': <azure.ai.ml.entities._job.job_service.JobService object at 0x155720fd0>}, 'comment': None, 'job_inputs': {'data': {'type': 'uri_file', 'path': 'azureml://subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourcegroups/RG/workspaces/AzureMLWorkspace/datastores/workspaceblobstore/paths/LocalUpload/3e91b5da3fd7435efb8a68549a7c027a/ril.csv', 'mode': 'ro_mount'}, 'test_train_ratio': '0.25', 'registered_model_name': 'stock_pred_v1', 'local_model_name': 'modelstock_pred_2023-01-25'}, 'job_outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.stock_pred_job_37', 'mode': 'rw_mount'}}, 'inputs': {'data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x155727f90>, 'test_train_ratio': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x1557212d0>, 'registered_model_name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x155727390>, 'local_model_name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x155725710>}, 'outputs': {'default': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x155727ed0>}, 'component': CommandComponent({'auto_increment_version': True, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': False, 'name': 'stock_pred_job_37', 'description': None, 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': PosixPath('.'), 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x155721e90>, 'serialize': <msrest.serialization.Serializer object at 0x155724e50>, 'command': 'python train.py --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} --local_model_name ${{inputs.local_model_name}} --registered_model_name ${{inputs.registered_model_name}}', 'code': '/subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourceGroups/RG/providers/Microsoft.MachineLearningServices/workspaces/AzureMLWorkspace/codes/c9998c88-2d8c-4693-95c2-aa8067661f8b/versions/1', 'environment_variables': {}, 'environment': '/subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourceGroups/RG/providers/Microsoft.MachineLearningServices/workspaces/AzureMLWorkspace/environments/stock-pred/versions/0.1.6', 'distribution': None, 'resources': None, 'version': None, 'latest_version': None, 'schema': None, 'type': 'command', 'display_name': 'stock_price_prediction', 'is_deterministic': True, 'inputs': {'data': {'type': 'uri_file', 'path': 'azureml://subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourcegroups/RG/workspaces/AzureMLWorkspace/datastores/workspaceblobstore/paths/LocalUpload/3e91b5da3fd7435efb8a68549a7c027a/ril.csv', 'mode': 'ro_mount'}, 'test_train_ratio': {'type': 'string', 'default': '0.25'}, 'registered_model_name': {'type': 'string', 'default': 'stock_pred_v1'}, 'local_model_name': {'type': 'string', 'default': 'modelstock_pred_2023-01-25'}}, 'outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.stock_pred_job_37', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {'status': 'Starting', 'parameters': {}}}), 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': {'Tracking': <azure.ai.ml.entities._job.job_service.JobService object at 0x15574fc50>, 'Studio': <azure.ai.ml.entities._job.job_service.JobService object at 0x155720fd0>}, 'status': 'Starting', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x155721e90>}, 'instance_id': '6ce3ed57-89ff-4c29-afa4-bec1ba625332', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': 'stock-pred:0.1.6', 'resources': {'instance_count': 1, 'shm_size': '2g'}, 'swept': False})"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model_name = f\"modelstock_pred_{str(datetime.now().date())}\"\n",
    "registered_model_name = \"stock_pred_v1\"\n",
    "\n",
    "job = command(   \n",
    "    name=f\"stock_pred_job_{next(x)}\",\n",
    "    inputs={\n",
    "        \"data\": Input(type=AssetTypes.URI_FILE, mode=\"ro_mount\", path=ril_data.path),\n",
    "        \"test_train_ratio\": 0.25,\n",
    "        \"registered_model_name\":registered_model_name,\n",
    "        \"local_model_name\":local_model_name\n",
    "        },\n",
    "    code=\"../src/\",  # location of source code\n",
    "    command=\"python train.py --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} --local_model_name ${{inputs.local_model_name}} --registered_model_name ${{inputs.registered_model_name}}\",\n",
    "    environment=env,\n",
    "    compute=compute.name,\n",
    "    experiment_name=\"train_model_stock_price_prediction\",\n",
    "    display_name=\"stock_price_prediction\",\n",
    ")\n",
    "\n",
    "ml_client.create_or_update(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_name=f\"modelstock_pred_{str(datetime.now().date())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modelstock_pred_2023-01-25'"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "        path=f\"azureml://jobs/{job.name}/outputs/artifacts/paths/outputs/\",\n",
    "        name=\"model-path\",\n",
    "        description=\"Model created from run.\",\n",
    "        type=\"custom_model\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model({'job_name': 'stock_pred_job_37', 'is_anonymous': False, 'auto_increment_version': False, 'name': 'model-path', 'description': 'Model created from run.', 'tags': {}, 'properties': {}, 'id': '/subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourceGroups/RG/providers/Microsoft.MachineLearningServices/workspaces/AzureMLWorkspace/models/model-path/versions/2', 'Resource__source_path': None, 'base_path': '/Users/anupam/Documents/Codebase/MLOps_stock_prediction/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x155720650>, 'serialize': <msrest.serialization.Serializer object at 0x15570b150>, 'version': '2', 'latest_version': None, 'path': 'azureml://subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourceGroups/RG/workspaces/AzureMLWorkspace/datastores/workspaceartifactstore/paths/ExperimentRun/dcid.stock_pred_job_37/outputs', 'datastore': None, 'utc_time_created': None, 'flavors': None, 'arm_type': 'model_version', 'type': 'custom_model'})"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.models.create_or_update(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src//deployment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {src_dir}/deployment.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "def init():\n",
    "    \n",
    "    global model\n",
    "    global scaler\n",
    "\n",
    "    class lstm_model(torch.nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super(lstm_model, self).__init__()\n",
    "            self.lstm1=torch.nn.LSTM(batch_first=True, input_size=5, hidden_size=1)\n",
    "            self.out=torch.nn.Linear(1,1)\n",
    "\n",
    "        def forward(self, x, hidden=None):\n",
    "            x, hidden = self.lstm1(x)\n",
    "            x = x[:,-1]\n",
    "            x = self.out(x)\n",
    "            return x, hidden\n",
    "\n",
    "    scalerpath = os.path.join(\n",
    "    os.getenv(\"AZUREML_MODEL_DIR\"), \"outputs/scaler.pkl\")\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    with open(scalerpath, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    modelpath = os.path.join(\n",
    "    os.getenv(\"AZUREML_MODEL_DIR\"), \"outputs/modelstock_pred_2023-01-25.pth\")    \n",
    "    model = lstm_model()\n",
    "    model.load_state_dict(torch.load(modelpath))\n",
    "    model.eval()\n",
    "\n",
    "def run(raw_data):\n",
    "\n",
    "    data = json.loads(raw_data)\n",
    "    data = np.array(list(data.values())).astype(float)\n",
    "    scaled_data = scaler.transform(data.reshape(-1,1))\n",
    "\n",
    "    tensor_data = torch.from_numpy(scaled_data.reshape(-1,1,5))\n",
    "\n",
    "    result, _ = model(tensor_data.float())\n",
    "\n",
    "    return result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x154e9c490>"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create endpoint\n",
    "online_endpoint_name = \"modeldir-logged-init\"\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"this is a sample online endpoint\"\n",
    ")\n",
    "\n",
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Succeeded'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_endpoints.get(name=online_endpoint_name).provisioning_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "modeldir = ml_client.models.get(name=\"model-path\", version=\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create deployment only after endpoint has provisioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint modeldir-logged-init exists\n",
      "Uploading src (0.01 MBs): 100%|██████████| 12465/12465 [00:01<00:00, 9791.13it/s]\n",
      "\n",
      "\n",
      "data_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x15598d310>"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................."
     ]
    }
   ],
   "source": [
    "# Deployment script\n",
    "code_config = CodeConfiguration(\n",
    "        code=src_dir, scoring_script=\"deployment.py\"\n",
    "    )\n",
    "\n",
    "# Create deployment\n",
    "yellow_deployment = ManagedOnlineDeployment(\n",
    "    name=\"yellow\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=modeldir,\n",
    "    environment=Environment(\n",
    "            conda_file=f\"./{dependencies_dir}/conda.yml\",\n",
    "            image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\"),\n",
    "    code_configuration=code_config,\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "# create the deployment:\n",
    "ml_client.begin_create_or_update(yellow_deployment)\n",
    "# blue deployment takes 100 traffic\n",
    "# endpoint.traffic = {\"yellow\": 100}\n",
    "# ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inputs.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile inputs.json\n",
    "{\"d1\":24.53,\"d2\":26.544,\"d3\":25,\"d4\":26.90,\"d5\":22}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[-0.024699201807379723]]'"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the endpoint (the request will route to blue deployment as set above)\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"yellow\",\n",
    "    request_file=\"inputs.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=ml_client.models.get(name=\"model-path\", version=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model({'job_name': 'stock_pred_job_34', 'is_anonymous': False, 'auto_increment_version': False, 'name': 'model-path', 'description': 'Model created from run.', 'tags': {}, 'properties': {}, 'id': '/subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourceGroups/RG/providers/Microsoft.MachineLearningServices/workspaces/AzureMLWorkspace/models/model-path/versions/1', 'Resource__source_path': None, 'base_path': '/Users/anupam/Documents/Codebase/MLOps_stock_prediction/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x152a31510>, 'serialize': <msrest.serialization.Serializer object at 0x152a32e90>, 'version': '1', 'latest_version': None, 'path': 'azureml://subscriptions/5d2e45e0-cd7b-4338-b279-455fa4a4c42d/resourceGroups/RG/workspaces/AzureMLWorkspace/datastores/workspaceartifactstore/paths/ExperimentRun/dcid.stock_pred_job_34/outputs', 'datastore': None, 'utc_time_created': None, 'flavors': None, 'arm_type': 'model_version', 'type': 'custom_model'})"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('inputs.json','rb') as f:\n",
    "    ccc = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['24.53', '26.544']"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ccc.values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployed endpoint logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Instance status:\\nSystemSetup: Succeeded\\nUserContainerImagePull: Succeeded\\nModelDownload: Succeeded\\nUserContainerStart: Succeeded\\n\\nContainer events:\\nKind: Pod, Name: Downloading, Type: Normal, Time: 2023-01-25T04:32:09.446943Z, Message: Start downloading models\\nKind: Pod, Name: Pulling, Type: Normal, Time: 2023-01-25T04:32:09.502906Z, Message: Start pulling container image\\nKind: Pod, Name: Pulled, Type: Normal, Time: 2023-01-25T04:32:19.1191Z, Message: Container image is pulled successfully\\nKind: Pod, Name: Downloaded, Type: Normal, Time: 2023-01-25T04:32:19.1191Z, Message: Models are downloaded successfully\\nKind: Pod, Name: Created, Type: Normal, Time: 2023-01-25T04:32:19.301497Z, Message: Created container inference-server\\nKind: Pod, Name: Started, Type: Normal, Time: 2023-01-25T04:32:19.411601Z, Message: Started container inference-server\\nKind: Pod, Name: ContainerReady, Type: Normal, Time: 2023-01-25T04:32:38.873661086Z, Message: Container is ready\\n\\nContainer logs:\\nListening at: http://127.0.0.1:31311 (12)\\nUsing worker: sync\\nworker timeout is set to 300\\nBooting worker with pid: 66\\nInitializing logger\\n2023-01-25 04:32:21,533 | root | INFO | Starting up app insights client\\nlogging socket was found. logging is available.\\nlogging socket was found. logging is available.\\n2023-01-25 04:32:21,533 | root | INFO | Starting up app insight hooks\\n2023-01-25 04:32:24,226 | root | INFO | Found user script at /var/azureml-app/src/Deployment.py\\n2023-01-25 04:32:24,227 | root | INFO | run() is not decorated. Server will invoke it with the input in JSON string.\\n2023-01-25 04:32:24,227 | root | INFO | Invoking user\\'s init function\\n00000000-0000-0000-0000-000000000000,/var/azureml-app/azureml-models/model-path/1/outputs/scaler.pkl\\n00000000-0000-0000-0000-000000000000,/var/azureml-app/azureml-models/model-path/1/outputs/modelstock_pred_2023-01-25.pth\\n2023-01-25 04:32:24,825 | root | INFO | Users\\'s init has completed successfully\\n2023-01-25 04:32:24,825 | root | ERROR | Encountered exception while generating swagger file: Traceback (most recent call last):\\n  File \"/var/azureml-server/aml_blueprint.py\", line 111, in _get_swagger\\n    from inference_schema.schema_util import (\\nImportError: cannot import name \\'get_supported_versions\\' from \\'inference_schema.schema_util\\' (/azureml-envs/azureml_886de9eab753e00d77cbed89865b3c2b/lib/python3.8/site-packages/inference_schema/schema_util.py)\\n\\n2023-01-25 04:32:24,826 | root | ERROR | Encountered exception while generating swagger file: Traceback (most recent call last):\\n  File \"/var/azureml-server/aml_blueprint.py\", line 111, in _get_swagger\\n    from inference_schema.schema_util import (\\nImportError: cannot import name \\'get_supported_versions\\' from \\'inference_schema.schema_util\\' (/azureml-envs/azureml_886de9eab753e00d77cbed89865b3c2b/lib/python3.8/site-packages/inference_schema/schema_util.py)\\n\\n2023-01-25 04:32:24,826 | root | INFO | Scoring timeout setting is not found. Use default timeout: 3600000 ms\\n2023-01-25 04:32:24,826 | root | INFO | AML_FLASK_ONE_COMPATIBILITY is set, but patching is not necessary.\\nSPARK_HOME not set. Skipping PySpark Initialization.\\n18678bd0-a655-4e20-924a-81329eb10dd1,/azureml-envs/azureml_886de9eab753e00d77cbed89865b3c2b/lib/python3.8/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\\n  warnings.warn(\\n2023-01-25 04:34:26,328 | root | ERROR | Encountered Exception: Traceback (most recent call last):\\n  File \"/var/azureml-server/user_script.py\", line 128, in invoke_run\\n    run_output = self._run(**run_parameters, request_headers=dict(request.headers))\\n  File \"/var/azureml-server/user_script.py\", line 154, in <lambda>\\n    self._run = lambda request_headers, **kwargs: self.user_module.run(**kwargs)\\n  File \"/var/azureml-app/src/Deployment.py\", line 47, in run\\n    result = model(tensor_data)\\nTypeError: \\'_IncompatibleKeys\\' object is not callable\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/var/azureml-server/routes.py\", line 215, in handle_score\\n    timed_result = app.user_script.invoke_run(request, timeout_ms=app.scoring_timeout_in_ms)\\n  File \"/var/azureml-server/user_script.py\", line 135, in invoke_run\\n    raise UserScriptException(ex) from ex\\nuser_script.UserScriptException: Caught an unhandled exception from the user script\\n\\n2023-01-25 04:34:26,328 | root | INFO | 500\\n127.0.0.1 - - [25/Jan/2023:04:34:26 +0000] \"POST /score HTTP/1.0\" 500 42 \"-\" \"azure-ai-ml/1.2.0 azsdk-python-core/1.26.2 Python/3.11.0 (macOS-13.0.1-arm64-arm-64bit)\"\\n\\n'"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_deployments.get_logs(\n",
    "    name=\"yellow\", endpoint_name=online_endpoint_name, lines=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "495794734634ed83a223a0ff466dd16659bfe4b87c179f1ac740ad96b06e37ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
