{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDK v1 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from azureml.core import Workspace, Experiment, Environment, ScriptRunConfig\n",
    "from azureml.core.compute import AmlCompute,ComputeTarget, ComputeInstance\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.environment import CondaDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(path='../../config/config.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=Environment.from_pip_requirements(\"conv_sum\",  '../config/requirements.txt')\n",
    "env.register(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ComputeManagement import create_cluster, create_instance, delete_compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc=create_cluster(\n",
    "    workspaceRef=ws,\n",
    "    name=\"q34\",\n",
    "    vmSize=\"Standard_DS3_v2\",\n",
    "    minNodes=0,\n",
    "    maxNodes=4,\n",
    "    idleTime=180\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_script_config = ScriptRunConfig(\n",
    "#     source_directory = 'src',\n",
    "#     script = 'training_script.py',\n",
    "#     arguments=['--data',___],\n",
    "#     environment = env,\n",
    "#     compute_target = cc\n",
    "# )\n",
    "# experiment = Experiment(\n",
    "#     workspace = ws,\n",
    "#     name=\"maiden_experiment\"\n",
    "# )\n",
    "# run = experiment.submit(config=training_script_config, tags=[])\n",
    "\n",
    "# RunDetails(run).show()\n",
    "# run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensexTickerYFinance = ['HDFCLIFE.NS, NESTLEIND.NS, KOTAKBANK.NS, INDUSINDBK.NS, TATASTEEL.NS, ITC.NS, ONGC.NS, TITAN.NS, ULTRACEMCO.NS, BAJAJFINSV.NS, BAJFINANCE.NS, BRITANNIA.NS, BAJAJ-AUTO.NS, COALINDIA.NS, BHARTIARTL.NS, TATACONSUM.NS, LTI.NS, CIPLA.NS, MARUTI.NS, ICICIBANK.NS, APOLLOHOSP.NS, NTPC.NS, HEROMOTOCO.NS, HINDALCO.NS, WIPRO.NS, TCS.NS, ADANIENT.NS, MM.NS, TECHM.NS, RELIANCE.NS']\n",
    "stock_data = yf.download(tickers=sensexTickerYFinance, start='2000-01-01', end='2022-12-31', interval='1mo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.loc[:,'Adj Close']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TODO\n",
    "1. Read about SOTA for stock price prediction and what determines success of model trying to predict price\n",
    "2. Choose stocks to monitor - Nifty 50\n",
    "3. Build as below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of a common use-case where data would update regularly and model would shift\n",
    "1. Stock price prediction\n",
    "2. Automatic data retrieval using API to store into Azure storage\n",
    "3. Automatic model training at intervals depending on error rate\n",
    "\n",
    "Tie everything up in a RL portfolio optimization application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickerData= yf.download(tickers=\"RELIANCE.NS\", start=\"2022-01-01\", end=\"2023-01-10\", period=\"1d\")\n",
    "tickerData['Date'] = [str(x)[:10] for x in tickerData.index]\n",
    "tickerData['Ticker'] = \"RELIANCE.NS\"\n",
    "tickerDataToPersist = list(tickerData.transpose().to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TickerData import query, download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(ticker=\"RELIANCE.NS\", start=\"2022-12-01\",end=\"2023-01-10\", period=\"1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = query(ticker=\"RELIANCE.NS\", start=\"2022-12-01\",end=\"2023-01-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker=\"RELIANCE.NS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[f\"{ticker}_Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(train_data).to_csv('./data/ril.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data(series, loookaheadSize=5):\n",
    "    X,y = [],[]\n",
    "    for i in np.arange(5,len(series)-1):\n",
    "        X.append(series[i-loookaheadSize:i])\n",
    "        y.append(series[i+1])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X.reshape(len(series)-loookaheadSize-1,1,5)\n",
    "    y=y.reshape(-1,1)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
    "\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx=training_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tx,'txx.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ = torch.load('txx.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(typ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0.0026]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "str(datetime.now().date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDK v2 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from azure.ai.ml import MLClient, Input, Output, command\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml.entities import AmlCompute, Environment, Model, Data, CodeConfiguration, ManagedOnlineEndpoint, ManagedOnlineDeployment\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config/config.json','r') as f:\n",
    "    configs=json.loads(f.read())\n",
    "\n",
    "subscription_id, resource_group, workspace = configs['subscription_id'], configs['resource_group'], configs['workspace_name']\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "ml_client = MLClient(\n",
    "    credential, subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = '../data/ril.csv'\n",
    "\n",
    "my_data = Data(\n",
    "    path=my_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"ril_stock_data\",\n",
    "    name=\"ril\",\n",
    "    version='1'\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ril_data = ml_client.data.get(name='ril', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ril_data.path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = \"../src/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {src_dir}train.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import mlflow\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def series_to_tensors(series, lookaheadSize=5):\n",
    "\n",
    "    X,y = [],[]\n",
    "    for i in np.arange(5,len(series)-1):\n",
    "        X.append(series[i-lookaheadSize:i])\n",
    "        y.append(series[i+1])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X.reshape(len(series)-lookaheadSize-1,1,5)\n",
    "    y=y.reshape(-1,1)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def dataprep(args):\n",
    "\n",
    "    stockData = pd.read_csv(args.data, index_col='Unnamed: 0')\n",
    "    stock_train_df, stock_test_df = train_test_split(stockData, test_size=args.test_train_ratio)\n",
    "\n",
    "    print(stock_train_df)\n",
    "\n",
    "    # Instead of this use LayerNorm or BatchNorm in the neural net\n",
    "    scaler = MinMaxScaler().fit(stock_train_df)\n",
    "    stock_train_df = scaler.transform(stock_train_df)\n",
    "    stock_test_df = scaler.transform(stock_test_df)\n",
    "\n",
    "    train_tensors = series_to_tensors(stock_train_df)\n",
    "    test_tensors = series_to_tensors(stock_test_df)\n",
    "\n",
    "    return scaler, train_tensors, test_tensors\n",
    "\n",
    "class lstm_model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(lstm_model, self).__init__()\n",
    "        self.lstm1=torch.nn.LSTM(batch_first=True, input_size=5, hidden_size=1)\n",
    "        self.out=torch.nn.Linear(1,1)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x, hidden = self.lstm1(x)\n",
    "        x = x[:,-1]\n",
    "        x = self.out(x)\n",
    "        return x, hidden\n",
    "\n",
    "def train(trainset):\n",
    "\n",
    "    seq_model = lstm_model()\n",
    "    optim = torch.optim.Adam(lr = 0.0001, params=seq_model.parameters())\n",
    "\n",
    "    epochs = 10\n",
    "\n",
    "    for epoch in np.arange(epochs):\n",
    "\n",
    "        Loss=0\n",
    "\n",
    "        for data in trainset:\n",
    "\n",
    "            feats, target = data\n",
    "            optim.zero_grad()\n",
    "\n",
    "            y_p,_ = seq_model(feats.float())\n",
    "            loss = torch.nn.functional.mse_loss(y_p.float(), target.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            Loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch: {epoch}, loss: {Loss}\")\n",
    "    return seq_model\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
    "    parser.add_argument(\"--data\", type=str, help=\"Path to input data\")\n",
    "    parser.add_argument(\"--local_model_name\", type=str, required=True, default=0.25)\n",
    "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load Scaler object later and send it for scaling data\n",
    "\n",
    "    scaler, trainset, _ = dataprep(args)\n",
    "\n",
    "    trainedModel = train(trainset)\n",
    "\n",
    "    print(os.getcwd())\n",
    "\n",
    "    pickle.dump(scaler, open('./outputs/scaler.pkl','wb'))\n",
    "    model_file = f\"./outputs/{args.local_model_name}.pth\"\n",
    "    torch.save(trainedModel.state_dict(), model_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def version_iter(n=20):\n",
    "    i=0\n",
    "    for i in np.arange(21,50):\n",
    "        yield i\n",
    "x = iter(version_iter())\n",
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_name = f\"modelstock_pred_{str(datetime.now().date())}\"\n",
    "registered_model_name = \"stock_pred_v1\"\n",
    "\n",
    "job = command(   \n",
    "    name=f\"stock_pred_job_{next(x)}\",\n",
    "    inputs={\n",
    "        \"data\": Input(type=AssetTypes.URI_FILE, mode=\"ro_mount\", path=ril_data.path),\n",
    "        \"test_train_ratio\": 0.25,\n",
    "        \"registered_model_name\":registered_model_name,\n",
    "        \"local_model_name\":local_model_name\n",
    "        },\n",
    "    code=\"../src/\",  # location of source code\n",
    "    command=\"python train.py --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} --local_model_name ${{inputs.local_model_name}} --registered_model_name ${{inputs.registered_model_name}}\",\n",
    "    environment=env,\n",
    "    compute=compute.name,\n",
    "    experiment_name=\"train_model_stock_price_prediction\",\n",
    "    display_name=\"stock_price_prediction\",\n",
    ")\n",
    "\n",
    "ml_client.create_or_update(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_name=f\"modelstock_pred_{str(datetime.now().date())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "        path=f\"azureml://jobs/{job.name}/outputs/artifacts/paths/outputs/\",\n",
    "        name=\"model-path\",\n",
    "        description=\"Model created from run.\",\n",
    "        type=\"custom_model\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.models.create_or_update(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {src_dir}/deployment.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "\n",
    "def init():\n",
    "    \n",
    "    global model\n",
    "    global scaler\n",
    "\n",
    "    class lstm_model(torch.nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super(lstm_model, self).__init__()\n",
    "            self.lstm1=torch.nn.LSTM(batch_first=True, input_size=5, hidden_size=1)\n",
    "            self.out=torch.nn.Linear(1,1)\n",
    "\n",
    "        def forward(self, x, hidden=None):\n",
    "            x, hidden = self.lstm1(x)\n",
    "            x = x[:,-1]\n",
    "            x = self.out(x)\n",
    "            return x, hidden\n",
    "\n",
    "    scalerpath = os.path.join(\n",
    "    os.getenv(\"AZUREML_MODEL_DIR\"), \"outputs/scaler.pkl\")\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    with open(scalerpath, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "\n",
    "    modelpath = os.path.join(\n",
    "    os.getenv(\"AZUREML_MODEL_DIR\"), \"outputs/modelstock_pred_2023-01-25.pth\")    \n",
    "    model = lstm_model()\n",
    "    model.load_state_dict(torch.load(modelpath))\n",
    "    model.eval()\n",
    "\n",
    "def run(raw_data):\n",
    "\n",
    "    data = json.loads(raw_data)\n",
    "    data = np.array(list(data.values())).astype(float)\n",
    "    scaled_data = scaler.transform(data.reshape(-1,1))\n",
    "\n",
    "    tensor_data = torch.from_numpy(scaled_data.reshape(-1,1,5))\n",
    "\n",
    "    result, _ = model(tensor_data.float())\n",
    "    \n",
    "    result = scaler.inverse_transform(result.detach().numpy())\n",
    "\n",
    "    return result.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create endpoint\n",
    "online_endpoint_name = \"modeldir-logged-init\"\n",
    "\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"this is a sample online endpoint\"\n",
    ")\n",
    "\n",
    "ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.get(name=online_endpoint_name).provisioning_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model\n",
    "modeldir = ml_client.models.get(name=\"model-path\", version=\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create deployment only after endpoint has provisioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment script\n",
    "code_config = CodeConfiguration(\n",
    "        code=src_dir, scoring_script=\"deployment.py\"\n",
    "    )\n",
    "\n",
    "# Create deployment\n",
    "yellow_deployment = ManagedOnlineDeployment(\n",
    "    name=\"yellow\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=modeldir,\n",
    "    environment=Environment(\n",
    "            conda_file=f\"./{dependencies_dir}/conda.yml\",\n",
    "            image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\"),\n",
    "    code_configuration=code_config,\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1,\n",
    ")\n",
    "# create the deployment:\n",
    "ml_client.begin_create_or_update(yellow_deployment)\n",
    "# blue deployment takes 100 traffic\n",
    "# endpoint.traffic = {\"yellow\": 100}\n",
    "# ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inputs.json\n",
    "{\"d1\":2663,\"d2\":2654.4,\"d3\":2698,\"d4\":2690,\"d5\":2698.12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the endpoint (the request will route to blue deployment as set above)\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"yellow\",\n",
    "    request_file=\"inputs.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.compute.begin_delete(name=compute.name).wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployed endpoint logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.get_logs(\n",
    "    name=\"yellow\", endpoint_name=online_endpoint_name, lines=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "495794734634ed83a223a0ff466dd16659bfe4b87c179f1ac740ad96b06e37ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
